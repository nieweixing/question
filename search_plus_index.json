{"./":{"url":"./","title":"简介","keywords":"","body":"简介 本书主要记录了个人运维过程中遇到的各种问题，希望这些笔记在大家学习的过程能帮到大家，相关笔记后续会逐步完善。 © vishon all right reserved，powered by GitbookUpdated at 2021-09-22 14:41:34 "},"tke/Pod-uses-components-to-mount-cfs-to-report-errors.html":{"url":"tke/Pod-uses-components-to-mount-cfs-to-report-errors.html","title":"1.pod采用组件挂载cfs报错","keywords":"","body":"pod采用组件挂载cfs报错 问题现象 pod通过cfs组件挂载cfs一直pending，pod起不来，查看事件报错MountVolume.MountDevice failed for volume \"xxxxxxx\" : kubernetes.io/csi: attacher.MountDevice failed to create newCsiDriverClient: driver name com.tencent.cloud.csi.cfs not found in the list of registered CSI drivers。 排查思路 根据事件日志报错是因为cfs组件agent异常导致，检查cfs组件的agent是正常运行。 查看事件发现有一条日志Starting pod sandbox eks-xxxxx，这个说明是调度到虚拟节点的，但是cfs组件的agent是csi-nodeplugin-cfsplugin(kube-system)，这个负载是DaemonSet类型，虚拟节点是无法运行DaemonSet，所以组件的agent无法在虚拟节点上运行。导致pod挂载cfs异常。 解决方案 将pod调度到正常节点上，不要调度到虚拟节点上，或者采用nfs的方式进行挂载。 © vishon all right reserved，powered by GitbookUpdated at 2021-10-20 23:34:42 "},"tke/Cannot-access-back-end-services-through-load-balancing-type-ingress.html":{"url":"tke/Cannot-access-back-end-services-through-load-balancing-type-ingress.html","title":"2.无法通过负载均衡类型ingress访问后端服务","keywords":"","body":"无法通过负载均衡类型ingress访问后端服务 问题现象 tke集群部署了一个nginx服务，并通过一个负载均衡类型的ingress暴露域名提供访问，但是通过域名无法访问到后端nginx服务。 排查思路 首先查看了ingress对应的clb监听后端是否正常，发现后端监听没有创建。 没有成功创建监听说明ingress同步规则到clb失败，这时候查看ingress事件是否有报错，看了下事件也没有具体报错。 事件不能看出来，这时候再去查看下ingress-controller日志，对应的pod是部署在集群内kube-system命名空间下的l7-lb-controller这个deploy，根据ingress名称搜下日志发现有报错，从报错中基本可以看出问题所在，一般主要存在下面几种情况。 1.后端的Service类型不匹配 sync ingress(coding01/nginx) error!err:Ingress Sync ClientError. ErrorCode: E4047 Details: Ingress: coding01/nginx. Service(coding01/nginx) is no able to be the backend of ingress. NodePort service or LoadBalancer service is support for ingress. OriginError: 2.ingress后端的svc被删除 2021-12-22T17:23:35.35178018+08:00 W1222 09:23:35.351592 1 controller.go:2729] sync ingress(coding01/nginx.) error!err:Ingress Sync ClientError. ErrorCode: E4041 Details: Ingress: coding01/nginx. Service(xxx/xxxx) is not found. OriginError: 解决方案 如果是service类型不匹配，需要将后端service为odePort或者LoadBalancer才行，这个问题只会出现在用yaml创建ingress，如果是控制创建的ingress是无法选择后端ClusterIP类型的service。 ingress后端svc被删除，需要将被删除的svc对应的ingress规则去掉，或者将svc重新创建好。 © vishon all right reserved，powered by GitbookUpdated at 2021-12-23 12:15:41 "},"tke/The-pod-in-the-tke-cluster-cannot-resolve-the-service-name.html":{"url":"tke/The-pod-in-the-tke-cluster-cannot-resolve-the-service-name.html","title":"4.tke集群内pod无法解析service name","keywords":"","body":"TKE集群内pod无法解析service name 问题现象 tke集群内的pod无法解析service name到cluserip上，服务之间访问出现异常。 本次问题现象就是在demo的pod无法解析同命名空间下的nginx这个service name，导致demo访问nginx异常。 排查思路 k8s集群内解析service name都是通过coredns来解析的，pod内会去请求coredns的service然后请求coredns的pod来解析service name，原理是这样，现在无法解析service name，是不是访问coredns的pod不通呢？ 进入demo直接访问coredns的pod，发现ping都不通，问题就出在这里，demo为什么无法访问coredns呢？要么就是coredns异常了，要么就是集群网络哪里有异常，这里看了下coredns的状态和日志都是正常的。 那说明是集群网络哪里有问题了，这里首先检查下节点的安全组，看下是否有问题。 查看了下安全组的入站和出站规则，出站都是放通，入站的话有放通vpc的网段，但是容器网段只放通了一个小的网段。 查看集群的容器网段发现是16的掩码，但是安全组只放通了26的掩码，这里安全组肯定是有问题。 解决方案 上面我们排查到了问题是出在安全组，这里我们将安全改下，放通全部容器网段看看。 放通全部容器网段后，再进入pod测试，发现可以正常解析域名了，并且也能正常访问coredns了。 集群内pod无法解析service-name问题就是节点安全组导致，入站没有放通全部容器网段导致的。 这里顺便解释下为什么要安全组放通容器网段，tke集群内默认部署了一个ip-masq-agent组件，这个组件的作用是决定pod访问哪些网段不做nat，默认配置的是容器网段和vpc网段，也就是说demo这个pod访问coredns不会做nat，coredns看到的是demo真实的客户端ip，也就是demo的pod-ip，当coredns和demo不在一个节点，就会跨节点访问，节点安全组没有放通demo所在的网段，导致demo无法访问coredns，最终在pod内的现象就是无法解析service name。 © vishon all right reserved，powered by GitbookUpdated at 2021-12-22 14:31:17 "},"k8s/The-hosts-configuration-in-coredns-cannot-be-correctly-parsed-in-pods-in-the-k8s-cluster.html":{"url":"k8s/The-hosts-configuration-in-coredns-cannot-be-correctly-parsed-in-pods-in-the-k8s-cluster.html","title":"1.k8s集群内pod内无法正确解析coredns中的hosts配置","keywords":"","body":"k8s集群内pod内无法正确解析coredns中的hosts配置 问题现象 k8s的coredns中进行了hosts的全局域名解析配置，但是在容器内解析对应的域名发现走的是k8s集群内部的svc.cluster.local域名，并没有解析到配置的hosts上 排查思路 这边首先测试了下在节点上配置了hosts是能正常解析，并且将pod的dns策略改成default不走coredns也是能正常解析，那么为什么经过coredns就不行了呢？这里看了下coredns中配置hosts的方式也没有问题。 这里查看了下coredns的官方文档hosts配置https://coredns.io/plugins/hosts/#description，发现我配置的hosts插件中没有设置fallthrough这个字段，根据文档的说明 fallthrough If zone matches and no record can be generated, pass request to the next plugin. If [ZONES…] is omitted, then fallthrough happens for all zones for which the plugin is authoritative. If specific zones are listed (for example and ), then only queries for those zones will be subject to fallthrough.in-addr.arpaip6.arpa 如果区域匹配并且无法生成记录，fallthrough则将请求传递给下一个插件，如果不设置，则不会继续找其他插件进行解析。 If you want to pass the request to the rest of the plugin chain if there is no match in the hosts plugin, you must specify the option.fallthrough 如果在hosts插件中没有匹配项的情况下要将请求传递给插件链的其余部分，则必须指定fallthrough，所以这里大概就是fallthrough没有设置导致的。 解决方案 coredns的hosts插件中加上fallthrough配置，然后重建coredns的pod即可。 © vishon all right reserved，powered by GitbookUpdated at 2021-10-20 23:55:31 "},"k8s/k8s-deploys-hostport-mode-pod-error-report.html":{"url":"k8s/k8s-deploys-hostport-mode-pod-error-report.html","title":"2.k8s部署hostport模式pod报错端口冲突","keywords":"","body":"k8s部署hostport模式pod报错端口冲突 最近有客户在使用hostport的模式部署deployment的时候，有出现这个错误0/4 nodes are available: 4 node(s) didn't have free ports for the requested pod ports，导致deployment部署失败，从报错看是端口冲突了。 因为hostport是用的占用的node节点的端口，既然是端口冲突，我们到节点上查看下对应端口是否有监听，登录节点用netstat查看对应端口的监听，发现并没有被占用，端口没有被占用，为什么会出现部署失败报错端口被占用的问题。 后面我根据报错的yaml进行测试下，具体现象如下 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: hostport-test qcloud-app: hostport-test name: hostport-test namespace: tke-test spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: hostport-test qcloud-app: hostport-test strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: hostport-test qcloud-app: hostport-test spec: containers: - image: nginx imagePullPolicy: Always name: hostport-test ports: - containerPort: 80 hostPort: 80 name: http protocol: TCP resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 部署了hostPort的pod后，我们登录节点查看80端口的监听，发现并没有被监听 [root@VM-0-10-centos ~]# netstat -an | grep tcp | grep 80 tcp 0 0 0.0.0.0:30080 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:32580 0.0.0.0:* LISTEN tcp 0 0 10.0.0.10:46548 169.254.0.71:80 ESTABLISHED tcp 0 0 10.0.0.10:33762 169.254.0.71:80 ESTABLISHED 接下来我们接着往hostport-test所在的节点部署一个hostport模式的pod看看 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: hostport-new qcloud-app: hostport-new name: hostport-new namespace: tke-test spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: hostport-new qcloud-app: hostport-new strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: hostport-new qcloud-app: hostport-new spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 10.0.0.10 containers: - image: nginx imagePullPolicy: Always name: hostport-new ports: - containerPort: 80 hostPort: 80 name: http protocol: TCP resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst hostNetwork: true imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 我们看下hostport-new的事件，看下是否有出现报错 [root@VM-0-10-centos ~]# kubectl describe pod -n tke-test hostport-new-86786bc46f-kl8ph Name: hostport-new-86786bc46f-kl8ph Namespace: tke-test ................. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal FailedSchedulingOnEkletNodes 7m49s (x16 over 12m) admission-controller node(s) didn't match node selector Warning FailedScheduling 65s (x10 over 12m) default-scheduler 0/5 nodes are available: 1 Too many pods, 2 node(s) didn't have free ports for the requested pod ports, 2 node(s) didn't match node selector. 从事件日志看，部署报错，也是报错端口没法使用，但是我们到节点查看80端口的确没有被监听，这是怎么回事呢？ 这里肯定是哪里配置不正常导致，我们仔细分析了下部署的yaml文件，发现hostport-test配置的hostport，但是没有配置hostNetwork，在k8s里面如果要hostport生效，需要配置hostNetwork为true才行，hostport-test没有配置hostNetwork，所以到节点上查看80端口没有被监听。 既然节点端口没被监听，为什么部署hostport-new这个pod时候还是会报错呢？hostport-new是有配置hostNetwork，其实从事件可以看出pod是调度失败，pod的调度是从etcd里面获取信息去判断节点是否存在相同端口的pod，也就是说当你有一个deployment存在hostport为80的字段时，Scheduler就会认为节点80端口已经被占用了，Scheduler不会实际去检查你的节点是否有这个端口的监听。 从上面的分析，就可以看出为什么节点没有起80端口的监听，但是部署hostport会报错端口被占用的报错。 © vishon all right reserved，powered by GitbookUpdated at 2021-07-19 11:44:58 "},"k8s/pod-mounting-nfs-data-volume-error-report-insufficient-permissions.html":{"url":"k8s/pod-mounting-nfs-data-volume-error-report-insufficient-permissions.html","title":"3.pod挂载nfs数据卷报错权限不足","keywords":"","body":"pod挂载nfs数据卷报错权限不足 k8s中将pod的数据持久化，很多时候我们会用到nfs，最近我们遇到一个pod挂载nfs报错的问题，具体报错如下 MountVolume.SetUp failed for volume \"xxxxxxx\" : mount failed: exit status 32 Mounting command: mount Mounting arguments: -t nfs 10.0.0.10:/data/xxxx /var/lib/eklet-agent/pods/d43cc0d5-b294-4040-9b1b-f9672d8a54e5/volumes/kubernetes.io~nfs/xxxxx Output: Created symlink /run/systemd/system/remote-fs.target.wants/rpc-statd.service → /usr/lib/systemd/system/rpc-statd.service. mount.nfs: Operation not permitted 从报错可以看出是权限不足，首先我们到nfs的服务端，检查下对应目录的权限，查看挂载目录的权限是/data/xxxx *(rw) nfs的权限是所有客户端都可以读写这个目录的，既然权限是足够，那么为什么会出现这个报错呢 通过google，发现有人对这个错误进行了定位和说明 由于pod出去的端口是高位端口，nfs默认是不允许高位端口去连接，所以需要加上insecure这个参数，这个参数的意思是 insecure：NFS通过1024以上的端口发送 最终的解决方案就是在nfs的/etc/exports文件将挂载的目录加上insecure参数 /data/xxxx *(rw),insecure © vishon all right reserved，powered by GitbookUpdated at 2021-07-19 13:31:09 "},"k8s/The-prestop-configuration-in-k8s-does-not-take-effect.html":{"url":"k8s/The-prestop-configuration-in-k8s-does-not-take-effect.html","title":"4.k8s中配置prestop不生效","keywords":"","body":"k8s中配置prestop不生效 k8s的prestop主要是用于pod的优雅停止，那么什么是优雅停止呢？ 何为优雅停止？ 优雅停止(Graceful shutdown)这个说法来自于操作系统，我们执行关机之后都得 OS 先完成一些清理操作，而与之相对的就是硬中止(Hard shutdown)，比如拔电源。 到了分布式系统中，优雅停止就不仅仅是单机上进程自己的事了，往往还要与系统中的其它组件打交道。比如说我们起一个微服务，网关把一部分流量分给我们，这时: 假如我们一声不吭直接把进程杀了，那这部分流量就无法得到正确处理，部分用户受到影响。不过还好，通常来说网关或者服务注册中心会和我们的服务保持一个心跳，过了心跳超时之后系统会自动摘除我们的服务，问题也就解决了；这是硬中止，虽然我们整个系统写得不错能够自愈，但还是会产生一些抖动甚至错误; 假如我们先告诉网关或服务注册中心我们要下线，等对方完成服务摘除操作再中止进程，那不会有任何流量受到影响；这是优雅停止，将单个组件的启停对整个系统影响最小化; 按照惯例，SIGKILL 是硬终止的信号，而 SIGTERM 是通知进程优雅退出的信号，因此很多微服务框架会监听 SIGTERM 信号，收到之后去做反注册等清理操作，实现优雅退出。 pod的停止流程 由于Pod所代表的是在集群中节点上运行的进程，当不再需要这些进程时允许其体面地 终止是很重要的。一般不应武断地使用KILL信号终止它们，导致这些进程没有机会完成清理操作。 设计的目标是令你能够请求删除进程，并且知道进程何时被终止，同时也能够确保删除操作终将完成。当你请求删除某个Pod时，集群会记录并跟踪Pod 体面终止周期， 而不是直接强制地杀死 Pod。在存在强制关闭设施的前提下， kubelet会尝试体面地终止 Pod。 通常情况下，容器运行时会发送一个TERM信号到每个容器中的主进程。 很多容器运行时都能够注意到容器镜像中 STOPSIGNAL 的值，并发送该信号而不是 TERM。 一旦超出了体面终止限期，容器运行时会向所有剩余进程发送 KILL信号，之后Pod就会被从API服务器上移除。如果kubelet或者容器运行时的管理服务在等待进程终止期间被重启， 集群会从头开始重试，赋予 Pod完整的体面终止限期。 Pod退出的流程大致如下： 1.用户删除 Pod。 2.1 Pod 进入 Terminating 状态。 2.2 与此同时，K8s 会将 Pod 从对应的 service 上摘除。 2.3 与此同时，针对有 PreStop Hook 的容器，kubelet 会调用每个容器的 PreStop Hook，假如 PreStop Hook 的运行时间超出了 grace period，kubelet 会发送 SIGTERM 并再等 2 秒。 2.4 与此同时，针对没有 PreStop Hook 的容器，kubelet 发送 SIGTERM。 3.grace period 超出之后，kubelet 发送 SIGKILL 干掉尚未退出的容器。 prestop配置不生效 为了服务做平滑升级或无损发布，我们会通过配置停止前的等待时间的prestop Hook，但是当我们sleep时间较长会发现prestop没有生效，其实pod的停止流程是有说明的 说明： 如果 preStop 回调所需要的时间长于默认的体面终止限期，你必须修改terminationGracePeriodSeconds属性值来使其正常工作。 terminationGracePeriodSeconds默认是30s，当我们的prostop配置的sleep 70s，就会出现pod不会等70s才被销毁，会马上kill掉，那么要怎么要配置才会生效呢？ 其实这里只需要将terminationGracePeriodSeconds时长修改比prestop所执行的时间长即可，也就是说你的prestop需要100s执行完成，你的terminationGracePeriodSeconds设置为101s就不会出现不生效的问题。 prestop配置多行命令 如何你的prestop想执行多条命令操作，可以参考下面配置 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - /bin/bash - '-c' - |- curl http://www.baidu.com sleep 70 curl http://www.qq.com © vishon all right reserved，powered by GitbookUpdated at 2021-12-08 19:11:37 "},"k8s/Docker-logs-and-kubectl-logs-view-logs-are-inconsistent.html":{"url":"k8s/Docker-logs-and-kubectl-logs-view-logs-are-inconsistent.html","title":"5.docker logs和kubectl logs查看日志不一致","keywords":"","body":"docker logs和kubectl logs查看日志不一致 问题现象 节点docker logs查看容器日志和命令kubectl logs查看日志有点不一致，kubectl logs查看的日志会少一部分。 排查思路 docker logs和kubectl logs查看日志都是查看/var/lib/docker/containers/[containerd_id]这个目录下的日志文件，既然是查看的结果不一致，说明是日志文件存在差异，进入这个目录查看了下 [root@node1 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554]# ll total 906480 drwx------ 2 root root 4096 Nov 1 17:33 checkpoints -rw------- 1 root root 16564 Nov 1 17:33 config.v2.json -rw-r----- 1 root root 28125126 Nov 5 15:47 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log -rw-r----- 1 root root 100000382 Nov 5 15:45 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.1 -rw-r----- 1 root root 100000267 Nov 5 15:40 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.2 -rw-r----- 1 root root 100000279 Nov 5 15:35 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.3 -rw-r----- 1 root root 100000078 Nov 5 15:28 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.4 -rw-r----- 1 root root 100000330 Nov 5 15:23 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.5 -rw-r----- 1 root root 100000788 Nov 5 15:18 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.6 -rw-r----- 1 root root 100000187 Nov 5 15:11 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.7 -rw-r----- 1 root root 100000916 Nov 5 15:06 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.8 -rw-r----- 1 root root 100003481 Nov 5 15:00 e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log.9 -rw-r--r-- 1 root root 2336 Nov 1 17:33 hostconfig.json drwx------ 2 root root 4096 Nov 1 17:33 mounts 从上面可以发现，容器的标准输出日志是有进行轮转的，这个是在/etc/docker/daemon.json的log-opts字段配置，既然kubectl logs查看的日志少了一部分，看了下kubectl logs日志最早时间是哪个，查看后发现和最早时间是e1a3e821aa55c9a77419a4bbb063c084f337336e4c46c3008be6752fbb8f8554-json.log这个文件的最开始的日志时间对应上，排查到这里，问题已经明确了，kubectl logs和docker logs查看的日志文件是不一样的，导致了日志有不同。 问题结论 kubectl logs查看的日志是xxxx-json.log这个日志文件的，而docker logs查看的是所有日志文件的日志，因此查看存在一定差异。 © vishon all right reserved，powered by GitbookUpdated at 2021-11-05 15:52:35 "},"istio/Cannot-connect-to-mysql-after-pod-injects-sidecar.html":{"url":"istio/Cannot-connect-to-mysql-after-pod-injects-sidecar.html","title":"1.pod注入sidecar后无法连接mysql","keywords":"","body":"pod注入sidecar后无法连接mysql 问题现象 业务pod注入了envoy之后无法连接mysql，去掉envoy注入后，连接mysql正常。 处理过程 看了下isito的版本是1.3.6，是属于老版本，查看集群service，发现集群中存在一个service也是3306端口，这个svc是一个http类型的，有个http filter， 流量在sidecar里是tcp数据，在 这个http filter里解码不出来，这是老版本的一个bug。 解决方案 将service的端口改成其他的，彻底解决需要升级isito版本。 © vishon all right reserved，powered by GitbookUpdated at 2021-11-05 18:32:35 "},"k8s/Alpine-mirror-cannot-access-redis-with-telnet.html":{"url":"k8s/Alpine-mirror-cannot-access-redis-with-telnet.html","title":"6.alpine镜像无法用telnet访问redis","keywords":"","body":"alpine镜像无法用telnet访问redis 问题现象 k8s中如果你启动的镜像操作系统是alpine，容器内无法用telnet连接redis。 测试过程 redis访问地址172.16.32.220，端口6379 分别用alpine和centos通过telnet测试连接redis 在alpine用redis-cli连接redis，排除alpine网络问题无法访问redis 解决方案 从上面的测试结果可以看出，alpine是能正常访问连接redis，然后centos也能telnet访问redis，只有在alpine无法telnet访问redis。 这个问题的原因，当前也没有具体的原因，怀疑是不同操作系统内核包不一样导致的。 这里的解决方案就是尽量避免在alpine通过telnet去测试redis是否正常或者网络是否通。 © vishon all right reserved，powered by GitbookUpdated at 2021-12-27 19:22:20 "}}